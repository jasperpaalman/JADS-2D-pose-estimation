import json
from typing import Dict, List, Tuple

import numpy as np
import matplotlib.pyplot as plt
import os
import time
import plotly.graph_objs as go
import pandas as pd

import cv2
from sklearn.cluster import DBSCAN

# pip install .whl file from https://www.lfd.uci.edu/~gohlke/pythonlibs/#opencv
# pip install numpy --upgrade if numpy.multiarray error

from itertools import chain
from itertools import groupby

from sklearn.metrics import mean_squared_error
from sklearn.metrics.pairwise import pairwise_distances
from sklearn.metrics.pairwise import euclidean_distances
from math import sqrt

connections = [
    (1, 2), (1, 5), (2, 3), (3, 4), (5, 6), (6, 7), (1, 8),
    (8, 9), (9, 10), (1, 11), (11, 12), (12, 13), (1, 0),
    (0, 14), (14, 16), (0, 15), (15, 17), (2, 16), (5, 17)
]


def get_openpose_output(location: str) -> List[List[Dict]]:
    """
    This function extracts all information generated by the openpose demo and converts it into a list of dictionaries.

    :param location: is the location of the .json files outputted by OpenPoseDemo.
    :return: A list of frames, each frame consists of a list of dictionaries in which
    all identified people in the video are described using the coordinates of the observed joints.
    """
    people_per_file = []
    # coordinate files are ordered, so we can iterate through the folder in which the coordinates are stores for a clip
    # each file corresponds to a frame
    for path, subdirs, files in os.walk(location):
        for filename in files:
            coord_path = os.path.join(path, filename)
            with open(coord_path) as f:
                people_per_file.append(json.load(f)['people'])
    return people_per_file


def determine_video_meta_data(file_path: str) -> Tuple:
    """"
    Extract the dimensions and frames per seconds from the video files

    :param file_path: Location of the video file
    :return: image_h image height, image_w image width, fps frames per second
    """
    cam = cv2.VideoCapture(file_path)
    ret_val, image = cam.read()
    image_h, image_w = image.shape[:2]  # getting clip resolution using opencv
    fps = cam.get(cv2.CAP_PROP_FPS)  # getting clip frames per second using opencv
    return image_h, image_w, fps


def rmse(a: List[float], b: List[float]) -> float:
    """"
    Function which determines the rmse error between two sets of coordinates points

    :param a: coordinates a
    :param b: coordinates b
    :return: The root mean squared error of the two coordinate sets.
    """
    return sqrt(mean_squared_error(a, b))


def calc_center_coords_of_person(person_coords, used_joints: List[int]) -> Tuple:
    """"
    Using the coordinates all coordinates of a person identified in a specific frame
    the 'centre' of this person is calculated

    :param used_joints: List of all joints that are identified by OpenPose for this person
    :param person_coords: Coordinates belonging to person

    :returns cx, xy: centre x and centre y coordinates.
    """
    cx = np.mean(person_coords[used_joints, 0])  # center x-coordinate
    cy = np.mean(person_coords[used_joints, 1])  # center y-coordinate
    return cx, cy


def determine_rmse_threshold(person_coords, used_joints: List[int]) -> float:
    """
    Using the centre coordinates of a identified person and the known used joints a rsme treshold is calculated

    :param used_joints: List of all joints that are identified by OpenPose for this person
    :param person_coords: Coordinates belonging to person
    :return rmse_threshold: a float value containing the rsme threshold
    """
    rmse_threshold = np.mean(
        pairwise_distances(np.array(calc_center_coords_of_person(person_coords, used_joints)).reshape(1, 2),
                           person_coords[used_joints, :2]))
    return rmse_threshold


def amount_of_frames_to_look_back(fps: float, frame: int) -> int:
    """
    Function that returns the amoount of frames that need be examined.

    :param fps: number of frames per second in current video
    :param frame: current frame in loop
    :return J: Number of frames to be examined
    """
    # number of frames to look back, set to 0.25 sec rather than number of frames
    max_frame_diff = int(fps // 4)
    if frame < max_frame_diff:
        j = frame
    else:
        j = max_frame_diff
    return j


def get_period_person_division(people_per_file: List[List[Dict]], fps: float) \
        -> Dict[int, Dict[int, any]]:
    """"


    :param fps: number of frames per second in current video
    :param people_per_file: A list of frames, each frame consists of a list of dictionaries in which
    all identified people in the video are described using the coordinates of the observed joints.

    :return period_person_division: data strucure containing per frame all persons and their
                                   corresponding coordinates

    """
    period_person_division = {}

    # used to create a new person when the algorithm can't find a good person fit based on previous x frames
    next_person = 0

    for frame, file in enumerate(people_per_file):
        # for a frame (period) make a new dictionary in which to store the identified people
        period_person_division[frame] = {}

        for person in file:
            # information for identifying people over disjoint frames
            person_coords = np.array([[x, -y, z] for x, y, z in np.reshape(person['pose_keypoints'], (18, 3))])

            best_person_fit = None  # Initially no best fit person in previous x frames is found
            if frame == 0:  # frame == 0 means no identified people exist (because first frame), so we need to create them ourselves
                period_person_division[frame][
                    next_person] = person_coords  # create new next people since it is the first frame
                next_person += 1
            else:
                # set sufficiently high rmse so it will be overwritten easily
                min_rmse = 1000

                # we don't want to base any computation on joints that are not present (==0), so we safe those indices that don't
                # contain any information
                empty_joints = set(np.where((person_coords == 0).all(axis=1))[0])

                # only select used joints
                used_joints = list(set(range(18)) - empty_joints)
                # set rmse_threshold equal to the mean distance of each used joint to the center
                rmse_threshold = determine_rmse_threshold(person_coords, used_joints)

                # for all possible previous periods within max_frame_diff
                for i in range(1, amount_of_frames_to_look_back(fps, frame) + 1):
                    for earlier_person in period_person_division[frame - i].keys():  # compare with all people
                        if earlier_person not in period_person_division[frame].keys():
                            # if not already contained in current period
                            earlier_person_coords = period_person_division[frame - i][earlier_person]
                            empty_joints_copy = empty_joints.copy()
                            empty_joints_copy = empty_joints_copy | set(
                                np.where((earlier_person_coords == 0).all(axis=1))[0])
                            used_joints = list(set(range(18)) - empty_joints_copy)
                            if len(used_joints) == 0:
                                continue
                            # compute root mean squared error based only on mutual used joints
                            person_distance = rmse(earlier_person_coords[used_joints, :], person_coords[used_joints, :])

                            # account for rmse threshold (only coordinates very close)
                            if person_distance < rmse_threshold:
                                if person_distance < min_rmse:  # if best fit, when compared to previous instances
                                    min_rmse = person_distance  # overwrite
                                    best_person_fit = earlier_person  # overwrite
                if best_person_fit is not None:  # if a best person fit is found
                    period_person_division[frame][best_person_fit] = person_coords
                else:  # else create new next person
                    period_person_division[frame][next_person] = person_coords
                    next_person += 1
    return period_person_division


def get_plottables_per_file(people_per_file: List[List[Dict]]) -> List[Dict[str, any]]:
    """

    :param people_per_file: So a list of frames, each frame consists of a list of dictionaries in which
    all identified people in the video are described using the coordinates of the observed joints.
    :return plottables_per_file:
    """
    plottables_per_file = []  # used for plotting all the coordinates and connected body part lines

    for frame, file in enumerate(people_per_file):

        plot_lines = []  # for plotting the entire video
        plot_coords = []  # for plotting the entire video
        plottables = {}  # new dictionary for a period, used for plotting entire video

        # coordinates of all people in this frame will be added to this list, to be iterated over later on
        # for plotting entire video
        coords = []

        # For plotting the entire video ###
        for person in file:
            # append coords for this frame/file for each person in the right format
            coords.append(np.array([[x, -y, z] for x, y, z in np.reshape(person['pose_keypoints'], (18, 3))]))

        for person_coords in coords:  # for all people in this frame
            plot_coords = plot_coords + list(
                person_coords[~(person_coords == 0).any(axis=1)])  # append present plottable coords

            # enumerate all x,y coordinate sets to be able to draw up lines
            # remove the ones that contain the value 0 --> joint not present
            coord_dict = {key: value for key, value in dict(enumerate(person_coords[:, :2])).items() if 0 not in value}

            present_keypoints = set(coord_dict.keys())  # only use joints that are present

            # get present connections: a connection contains 2 unique points, if a connection contains one of the keypoints that
            # is not present, the intersection of the connection with the present keypoints will be lower than 2
            # hence we end up with only the present connections
            present_connections = [connection for connection in connections if
                                   len(present_keypoints & set(connection)) == 2]

            # gather the connections, change the layout to fit matplotlib and extend the plot_lines list
            plot_lines = plot_lines + [np.transpose([coord_dict[a], coord_dict[b]]) for a, b in present_connections]

        if len(plot_coords) == 0:
            continue

        plot_coords = np.array(plot_coords)  # for easy indexing

        plottables['plot_coords'] = plot_coords
        plottables['plot_lines'] = plot_lines

        plottables_per_file.append(
            plottables)  # append plottables_per_file with the plottables dictionary for this frame
    return plottables_per_file


def plot_fit(plottables_per_file: List[Dict[str, any]], period: int, f, ax, image_w: float, image_h: float) -> None:
    """

    :param plottables_per_file:
    :param period:
    :param f: The Mathplotlib figure to draw on
    :param ax: Matplotlib axes, to fit to.
    :param image_w:
    :param image_h:
    :return:
    """
    plot_coords = plottables_per_file[period]['plot_coords']
    plot_lines = plottables_per_file[period]['plot_lines']
    plt.interactive(False)

    plt.scatter(x=plot_coords[:, 0], y=plot_coords[:, 1])

    for x, y in plot_lines:
        plt.plot(x, y)

    ax.set_xlim([0, image_w])
    ax.set_ylim([-image_h, 0])

    f.canvas.draw()
    ax.clear()


def get_person_period_division(period_person_division: Dict[int, Dict[int, any]]) \
        -> Dict[int, Dict[int, np.ndarray]]:
    """
    Function that reverses the indexing in the dictionary
    :param period_person_division: data strucure containing per frame all persons and their
                                   corresponding coordinates
    :return person_period_division: data structure containing per person all frames and the coordinates
                                    of that person in that frame.
    """
    person_period_division = {}
    for person in set(chain.from_iterable(period_person_division.values())):
        person_period_division[person] = {}
        for period in period_person_division.keys():
            period_dictionary = period_person_division[period]
            if person in period_dictionary:
                person_period_division[person][period] = period_dictionary[person]
    return person_period_division


def get_mean_x_per_person(person_period_division: Dict[int, Dict[int, any]]) \
        -> Dict[int, Dict[int, float]]:
    """
    Calculate the mean x-position of a person in a certain period

    :param person_period_division:
    :returns: a dictionary
    """
    return {person: {period: np.mean(coords[~(coords == 0).any(axis=1), 0])
                     for period, coords in time_coord_dict.items()}
            for person, time_coord_dict in person_period_division.items()}


def normalize_moved_distance_per_person(mean_x_per_person: Dict[int, Dict[int, any]]) -> Dict[int, int]:
    """
    Calculate moved distance by summing the absolute difference over periods
    Normalize moved distance per identified person over frames by including the average frame difference and the length
    of the number of frames included

    :param mean_x_per_person: A Persons containing their frames containing the mean x for that person for that frame.
    :return:
    """
    normalized_moved_distance_per_person = \
        {person: pd.Series(mean_x_dict).diff().abs().sum() / (
                np.diff(pd.Series(mean_x_dict).index).mean() * len(mean_x_dict))
         for person, mean_x_dict in mean_x_per_person.items()}

    return {key: value for key, value in normalized_moved_distance_per_person.items() if
            value == value}


def get_person_plottables_df(mean_x_per_person: Dict[int, Dict[int, any]], moving_people: List[int]) -> pd.DataFrame:
    """
    Finding person under observation based on clustering with DBSCAN

    :param mean_x_per_person:
    :param moving_people:
    :return:
    """
    return pd.DataFrame(
        [(period, person, x) for person, period_dict in mean_x_per_person.items() if person in moving_people
         for period, x in period_dict.items()], columns=['Period', 'Person', 'X mean'])


def get_dbscan_subsets(maximum_normalized_distance: float, person_plottables_df: pd.DataFrame):
    """

    :param maximum_normalized_distance:
    :param person_plottables_df:
    :return:
    """
    db = DBSCAN(eps=maximum_normalized_distance, min_samples=1)

    db.fit(person_plottables_df[['Period', 'X mean']])

    person_plottables_df['labels'] = db.labels_

    # maximum_label = person_plottables_df.groupby('labels').apply(len).sort_values(ascending=False).index[0]

    dbscan_subsets = person_plottables_df.groupby('labels')['Person'].unique().tolist()

    return [list(i) for i in dbscan_subsets]


def iterative_main_traject_finder(person_plottables_df: pd.DataFrame,
                                  plottable_people: any,
                                  period: int,
                                  x: List,
                                  y: List,
                                  max_rmse: float,
                                  max_dist: float) -> Tuple[List, List, any]:
    """
    Given a period that needs to be tested and some x,y coordinate set to extrapolate from, this function tests,
    based on the maximum RMSE, if the point(s) within this period are comparable with the current region.
    The x,y coordinates are returned as well as the updated plottable people set.

    TODO: improve type notation for x, y and define purpose of parameters (define type of plottable people)

    :param max_dist:
    :param person_plottables_df:
    :param plottable_people:
    :param period:
    :param x:
    :param y:
    :param max_rmse:
    :return:
    """

    best_point = None
    dist_point = None

    z = np.polyfit(x, y, 10)  # fit polynomial with sufficient degree to the datapoints
    f = np.poly1d(z)

    # retrieve values that belong to this period (can contain more than one point, when noise is present)
    period_selection = person_plottables_df[person_plottables_df['Period'] == period][
        ['Period', 'Person', 'X mean']].values

    # for each of these points check the RMSE
    for period, person, x_mean in period_selection:
        rmse_period = rmse([f(period)], [x_mean])
        if rmse_period < max_rmse:
            max_rmse = rmse_period
            best_point = (period, x_mean, person)
        elif euclidean_distances([[period, x_mean]], list(zip(x, y))).min() < max_dist:
            dist_point = (period, x_mean, person)

    if best_point is not None:
        x.append(best_point[0])
        y.append(best_point[1])
        plottable_people = plottable_people | {int(best_point[2])}

    elif dist_point is not None:
        x.append(dist_point[0])
        y.append(dist_point[1])
        plottable_people = plottable_people | {int(dist_point[2])}

    return x, y, plottable_people


def determine_plottable_people(
        person_plottables_df: pd.DataFrame,
        max_dbscan_subset: List,
        max_rmse: float,
        max_dist: float) -> any:
    """
    This function takes the largest DBSCAN subset as a starting point and starts expanding to periods that are not
    yet covered.

    For each period not covered yet, the points that are already included are used to create a polynomial
    function to extrapolate from. The points contained within the period are compared and one or zero points can be
    chosen to be included in the main traject/region, which depends on the maximum RMSE that is set.

    If RSME of no point for a period lies below the maximum RMSE,
    no point is included and we move over to the next period in line
    The periods lower than the initially covered region by DBSCAN is indicated as the lower_periods,
    the periods higher as the upper_periods.

    TODO: Define type of max_dbscan_subset elements, define use of variables.

    :param max_dist:
    :param max_rmse:
    :param max_dbscan_subset:
    :param person_plottables_df:


    :return plottable_people:
    """

    plottable_people = set(max_dbscan_subset)  # set-up plottable people set

    # Make a selection of the dataframe that is contained within the current initial region
    df_sel = person_plottables_df[person_plottables_df['Person'].isin(max_dbscan_subset)].sort_values('Period')

    x = df_sel['Period'].tolist()  # starting x list
    y = df_sel['X mean'].tolist()  # starting y list

    # Region lower and upper bound
    region_lower_bound = person_plottables_df[person_plottables_df['Person'] == min(max_dbscan_subset)]['Period'].min()
    region_upper_bound = person_plottables_df[person_plottables_df['Person'] == max(max_dbscan_subset)]['Period'].max()

    # Determine lower and upper periods to cover
    lower_periods = set(range(person_plottables_df['Period'].min(), region_lower_bound)) & set(
        person_plottables_df['Period'])
    upper_periods = set(range(region_upper_bound + 1, person_plottables_df['Period'].max())) & set(
        person_plottables_df['Period'])

    for period in upper_periods:
        x, y, plottable_people = \
            iterative_main_traject_finder(person_plottables_df, plottable_people, period, x, y, max_rmse, max_dist)

    for period in list(lower_periods)[::-1]:
        x, y, plottable_people = \
            iterative_main_traject_finder(person_plottables_df, plottable_people, period, x, y, max_rmse, max_dist)

    return plottable_people


def get_running_and_turning_fragments(
        plottable_people: List[int],
        mean_x_per_person: Dict,
        person_plottables_df,
        moving_people,
        fps: float):
    """
    Given the identified plottable people (running person/person under observation), this function returns the
    divided running and turning fragments. That is, each running fragment is a person running from one side to the other
    and the turning fragments are the fragments that remain.

    :param fps: The frame rate of the video
    :param plottable_people: The indices of identified 'people' that belong to the running person
    :param mean_x_per_person: Mean x-position of a person in a certain period (average over all joints)
    :param person_plottables_df: Dataframe that contains plottable information for all moving people
        (running person + noise)
    :param moving_people: All 'people' that have a normalized moved distance that exceeds a set threshold.
        Contains the running person and possibly noise.

    :returns running_fragments, turning_fragments: Both are a list of tuples. Each tuple indicated the start frame and
        end frame of a fragment.
        Running fragments indicate the estimated fragments where the person under observation is running
        Turning fragments indicate the estimated fragments where the person is either slowing down, turning or starting,
        i.e. not solely running
    """

    # Plot the original dataframe to show the difference between moving_people (incl. noise)
    # and the extract plottable_people
    pd.DataFrame({key: value for key, value in mean_x_per_person.items() if key in moving_people}).plot()

    # Retrieve dataframe, but only select plottable people
    plottable_people_df = person_plottables_df[person_plottables_df['Person'].isin(plottable_people)].sort_values(
        'Period')

    x = plottable_people_df['Period'].values
    y = plottable_people_df['X mean'].values

    min_period = plottable_people_df['Period'].min()  # minimum period/frame
    max_period = plottable_people_df['Period'].max()  # maximum period/frame

    # fit polynomial with sufficient degree to the datapoints
    z = np.polyfit(x, y, 20)
    f = np.poly1d(z)

    # Construct new smooth line using the polynomial function
    # Number of points are the number of periods times a multiplication factor
    xnew = np.linspace(min_period, max_period, num=len(x) * 10, endpoint=True)
    ynew = f(xnew)

    # Determine optima indexes for xnew and ynew
    # Function checks if there is a sign change and if sufficient points (# indicated through 'periods' variable)
    # surrounding this candidate turning point are not changing in sign
    periods = int(fps * 10)
    periods_sign_diff = np.diff(np.sign(np.diff(ynew)))
    optima_ix = [i + 1 for i in range(len(periods_sign_diff)) if periods_sign_diff[i] != 0
                 and (periods_sign_diff[i - periods:i] == 0).all()
                 and (periods_sign_diff[i + 1:i + periods + 1] == 0).all()]  # local min+max

    # The optima reflect the turning points, which can be retrieved in terms of frames
    turning_points = xnew[optima_ix]
    turning_points = list(
        map(lambda t: min(plottable_people_df['Period'].unique(), key=lambda x: abs(x - t)), turning_points))

    # Add minimum and maximum period/frame of the interval we look at
    turning_points = sorted(set(turning_points) | set([min_period, max_period]))

    # Locate the x-mean values that belong to these points
    z = np.polyfit(x, y, 10);
    f = np.poly1d(z)
    turning_x = list(f(turning_points))

    # Find the relevant points by checking if it is the minimum and maximum period/frame of the interval or
    # if the points are sufficiently apart in both x and y coordinate
    points_x = []
    points_y = []
    for i, point in enumerate(turning_points):
        if point in [min_period, max_period]:
            points_x.append(point)
            points_y.append(turning_x[i])
        else:
            if abs(turning_x[turning_points.index(points_x[-1])] - turning_x[i]) > 200:
                points_x.append(point)
                points_y.append(turning_x[i])

    # Derive fragments
    fragments = [(i, j) for i, j in zip(points_x, points_x[1:]) if j - i > fps]

    # Plot found information
    plt.plot(xnew, ynew)
    plt.plot(points_x, points_y, "o", color='orange')
    plt.title('Finding turning points (optima) and deriving fragments')
    plt.xlabel('Frames')
    plt.ylabel('X position')
    plt.legend().set_visible(False)

    # Plot coordinates that will be used in further analyses + identified slow down points
    pd.DataFrame({key: value for key, value in mean_x_per_person.items() if key in plottable_people}).plot()
    plt.title(
        'All coordinates that will be used in further analyses & \n identified points where a start or a slow-down is finalized')
    plt.xlabel('Frames')
    plt.ylabel('X position')
    plt.legend().set_visible(False)

    ### Splitting turning and running motions ###

    plottable_people_df.set_index('Period')

    turning_frames = []

    # Individually look at each fragment (this contains parts that are a running motion and parts that are a turning/starting motion)
    # Rationale: Check for maximum change in accelertion by taking minimum/maximum of second derivative

    avg_first_derivative = np.mean(abs(np.diff(ynew)))  # average first derivative
    avg_second_derivative = np.mean(abs(np.diff(np.diff(ynew))))  # average second derivative

    for fragment in fragments:
        # mask for selecting the coordinates of each fragment
        mask = (xnew >= fragment[0]) & (xnew < fragment[1])

        # selection of frames (x_sel) and x-position (y_sel)
        x_sel = xnew[mask]
        y_sel = ynew[mask]

        # turn into series
        fragment_series = pd.Series(y_sel, index=x_sel)

        periods = fragment_series.index.tolist()

        # We split the fragment in a x number of parts and look only at the middle part (i.e. interval [1/x, x-1/x] )
        # In this interval we will find the deviations in both sides from second derivative == 0 and mark those as the start and end point of a running fragment
        split = 5
        lower_bound = periods[len(periods) // split]
        mid = periods[len(periods) // 2]
        upper_bound = periods[(len(periods) // split) * (split - 1)]

        secondDerivative = fragment_series.diff().diff()  # calculating second derivative

        # print(fragment[0], lower_bound, mid, upper_bound, fragment[1])

        # plt.figure()
        # secondDerivative.plot()

        # At second derivative == 0, a person reaches his/her top speed
        # Around this point we set a certain confidence bound, for which we can be fairly sure that a person is running
        # Since the model tended to include slowing down a lot, a penalty is added relative to the start-up part
        # The map function makes sure that when no value falls within the interval that the upper or lower bound is assigned

        start_up_thresh = avg_second_derivative
        slow_down_thresh = (2 / 5) * start_up_thresh

        if y_sel[0] < y_sel[-1]:
            min_frame = secondDerivative[(fragment_series.diff().abs() > avg_first_derivative) & (
                secondDerivative.apply(lambda x: 0 < x < start_up_thresh))].index.min()
            min_frame = list(map(lambda x: lower_bound if x != x else x, [min_frame]))[0]

            max_frame = secondDerivative[(fragment_series.diff().abs() > avg_first_derivative) & (
                secondDerivative.apply(lambda x: -slow_down_thresh < x < 0))].index.max()
            max_frame = list(map(lambda x: upper_bound if x != x else x, [max_frame]))[0]

            turning_frames.append(min_frame)
            turning_frames.append(max_frame)
        else:
            min_frame = secondDerivative[(fragment_series.diff().abs() > avg_first_derivative) & (
                secondDerivative.apply(lambda x: -start_up_thresh < x < 0))].index.min()
            min_frame = list(map(lambda x: lower_bound if x != x else x, [min_frame]))[0]

            max_frame = secondDerivative[(fragment_series.diff().abs() > avg_first_derivative) & (
                secondDerivative.apply(lambda x: 0 < x < slow_down_thresh))].index.max()
            max_frame = list(map(lambda x: upper_bound if x != x else x, [max_frame]))[0]

            turning_frames.append(min_frame)
            turning_frames.append(max_frame)

    # get the estimated x-position where a start or a slow-down is finalized (just for plotting purposes)
    z = np.polyfit(x, y, 10);
    f = np.poly1d(z)
    turning_x = f(turning_frames)

    # Adding points to plot where a start or a slow-down is finalized (just for plotting purposes)
    plt.scatter(turning_frames, turning_x)

    all_points = sorted(set(turning_frames) | set([min_period, max_period]))
    all_fragments = list(zip(all_points, all_points[1:]))

    # Splitting running fragments and turning fragments
    running_fragments = all_fragments[1::2]
    turning_fragments = all_fragments[::2]

    return running_fragments, turning_fragments, fragments


def get_clip_stats(clip_name):
    '''Get clip stats, when provided. Else return None.'''

    distances = ['16500']

    clip_name_split = clip_name.split('_')
    if clip_name_split[-1] in distances:
        length = int(clip_name_split[-3])
        weight = int(clip_name_split[-2])
        distance = int(clip_name_split[-1])
        return length, weight, distance
    else:
        return None


def euclidean_pairwise_distance(matrix):
    '''Given a matrix, calculates the pairwise distance between two rows. If the number of rows is not equal to 2 NaN is returned'''

    if matrix.shape[0] != 2:
        return np.nan

    return pairwise_distances(matrix[0].reshape(1, -1), matrix[1].reshape(1, -1))


def get_person_length_in_pixels(person_plottables, length, joint_confidence):
    '''Given the provided length of a person and some confidence bound on each joint ('gewricht' in Dutch) returns a measurement
    of a persons length in pixel values.'''

    # z value in the x,y,z coordinate output. Set a threshold to only include fairly certain coords

    # find all the coordinates of the person that are not empty and that exceed a set confidence level
    coord_list = [np.concatenate((np.arange(18).reshape(-1, 1), coords), axis=1)[(~(coords == 0).any(axis=1))
                                                                                 & (coords[:, 2] > joint_confidence)]
                  for period, person_dictionary in person_plottables.items()
                  for person, coords in person_dictionary.items()]

    # Check out: https://github.com/CMU-Perceptual-Computing-Lab/openpose/blob/master/doc/media/keypoints_pose.png
    connections = [(0, 1), (1, 8), (1, 11), (8, 9), (11, 12), (9, 10),
                   (12, 13)]  # connections used for estimating length in pixels

    connection_lengths = []  # will contain averaged pixel length of the connections

    for connection in connections:
        connection_length = np.nanmean([euclidean_pairwise_distance(coords[np.isin(coords[:, 0], connection)][:, 1:3])
                                        for coords in coord_list])

        connection_lengths.append(connection_length)

    pixel_length = connection_lengths[0] + sum([np.mean([connection_lengths[i], connection_lengths[i + 1]])
                                                for i in range(len(connections))[1::2]])

    return pixel_length


def speed_via_length(person_plottables, running_fragments, length, fps, joint_confidence=0.5):
    '''Returns estimated speed in km/h per running fragment by using provided length as inference measurement.'''

    pixel_length = get_person_length_in_pixels(person_plottables, length, joint_confidence)
    length_in_meters = length / 100

    pixel_length_ratio = length_in_meters / pixel_length

    speed = []

    for start, end in running_fragments:
        start = int(round(start, 0))
        end = int(round(end, 0))

        start_x = np.nanmean(
            [np.mean(coords[~(coords == 0).any(axis=1)][:, 0]) for coords in person_plottables[start].values()])
        end_x = np.nanmean(
            [np.mean(coords[~(coords == 0).any(axis=1)][:, 0]) for coords in person_plottables[end].values()])

        x_diff = abs(end_x - start_x)

        meters_diff = pixel_length_ratio * x_diff

        fragment_speed = meters_diff / ((end - start) / fps) * 3.6

        speed.append(fragment_speed)

    return speed


def speed_via_distance(person_plottables, running_fragments, fragments, fps, distance=16500):
    '''Returns estimated speed in km/h per running fragment by using provided distance as inference measurement.'''

    distance_in_meters = distance / 1000

    bounds = []

    for start, end in fragments[1:3]:
        start_x = np.nanmean(
            [np.mean(coords[~(coords == 0).any(axis=1)][:, 0]) for coords in person_plottables[start].values()])
        end_x = np.nanmean(
            [np.mean(coords[~(coords == 0).any(axis=1)][:, 0]) for coords in person_plottables[end].values()])

        bounds = bounds + [start_x, end_x]

    lower_bound = min(bounds)
    upper_bound = max(bounds)

    pixel_distance = upper_bound - lower_bound

    pixel_distance_ratio = distance_in_meters / pixel_distance

    speed = []

    for start, end in running_fragments:
        start = min(person_plottables.keys(), key=lambda x: abs(x - start))
        end = min(person_plottables.keys(), key=lambda x: abs(x - end))

        start_x = np.nanmean(
            [np.mean(coords[~(coords == 0).any(axis=1)][:, 0]) for coords in person_plottables[start].values()])
        end_x = np.nanmean(
            [np.mean(coords[~(coords == 0).any(axis=1)][:, 0]) for coords in person_plottables[end].values()])

        x_diff = abs(end_x - start_x)

        meters_diff = pixel_distance_ratio * x_diff

        fragment_speed = meters_diff / ((end - start) / fps) * 3.6

        speed.append(fragment_speed)

    return speed


def plot_person(plottables, image_h, image_w, zoom=True, pad=3, sleep=0):
    """
    :param ax:
    :param f:
    :param plottables:
    """
    f, ax = plt.subplots(figsize=(14, 10))

    y_coords = [coords[~(coords == 0).any(axis=1)][:, 1]
                for period_dictionary in plottables.values() for coords in period_dictionary.values()]

    y_coords = list(chain.from_iterable(y_coords))

    cy = np.mean(y_coords)  # y center
    stdy = np.std(y_coords)  # y standard deviation

    ydiff = stdy * pad * 2  # total range of y

    aspect = image_w / image_h

    for t in sorted(plottables.keys()):

        for person in plottables[t].keys():
            plot_coords = plottables[t][person]

            coord_dict = {key: value for key, value in dict(enumerate(plot_coords[:, :2])).items() if 0 not in value}

            present_keypoints = set(coord_dict.keys())

            present_connections = [connection for connection in connections if
                                   len(present_keypoints & set(connection)) == 2]

            plot_lines = [np.transpose([coord_dict[a], coord_dict[b]]) for a, b in present_connections]

            plot_coords = plot_coords[~(plot_coords == 0).any(axis=1)]

            plt.scatter(x=plot_coords[:, 0], y=plot_coords[:, 1])

            for x, y in plot_lines:
                plt.plot(x, y)

            ax.annotate('Frame: {}'.format(t), xy=(0.02, 0.95), xycoords='axes fraction',
                        bbox=dict(facecolor='red', alpha=0.5), fontsize=12)

            if zoom:
                ax.set_ylim(cy - stdy * pad, cy + stdy * pad)  # set y-limits by padding around the average center of y
                xlow, xhigh = ax.get_xlim()  # get x higher and lower limits
                xdiff = xhigh - xlow  # calculate the total range of x
                xpad = ((
                                ydiff * aspect) - xdiff) / 2  # calculate how much the xlimits should be padded on either side to set aspect ratio correctly
                ax.set_xlim(xlow - xpad, xhigh + xpad)  # set new limits
            else:
                ax.set_xlim([0, image_w])
                ax.set_ylim([-image_h, 0])

            f.canvas.draw()
            ax.clear()

            break

        time.sleep(sleep)


# Plotting coordinates of joints
def prepare_data_for_plotting(period_person_division, plottable_people, running_fragments):
    """"
    Returns a list of all coordinates

    :param period_person_division:
    :param plottable_people:
    :param running_fragments:

    :return coord_list:
    """
    coord_list = []
    for n, running_fragment in enumerate(running_fragments):
        coord_list.append([])
        for period, period_dictionary in period_person_division.items():
            for person, coords in period_dictionary.items():
                if person in plottable_people and running_fragment[0] <= period < running_fragment[1]:
                    coord_dict = {key: value for key, value in dict(enumerate(coords[:, :2])).items() if 0 not in value}
                    coord_list[n].append(coord_dict)
                    break
    return coord_list


# To dataframe

def get_dataframe_from_coords(coord_list):
    """"
    Get a list of coordinates and turns this into a DataFrame to be used for analysis

    :param coord_list: List of List of dictionaries containing coordinates the run both ways.
    :return coord_df: A DataFrame containing all x and y coordinates of the runner during the run.

    The for loop when the 'Fragment' = i+1 is done should become a double for loop, also naming the video number, when adding more videos
    """

    # More robust way of creating the coord_df
    coord_df = pd.DataFrame(
        [(n, frame, ix, *coords) for n, coord_list in enumerate(coord_list) for frame, coord_dict in
         enumerate(coord_list)
         for ix, coords in coord_dict.items()], columns=['Fragment', 'Frame', 'Point', 'x', 'y'])

    # Numeric to name dictionary
    replace_dict = dict(enumerate(['Nose', 'Neck', 'Right Shoulder', 'Right Elbow', 'Right Hand',
                                   'Left Shoulder', 'Left Elbow', 'Left Hand', 'Right Hip',
                                   'Right Knee', 'Right Foot', 'Left Hip', 'Left Knee',
                                   'Left Foot', 'Right Eye', 'Left Eye', 'Right Ear', 'Left Ear']))

    # Turn numerics into names
    coord_df['Point'] = coord_df['Point'].replace(replace_dict)

    return coord_df


def to_feature_df(coord_df, video_number):
    """
    Gets a DataFrame of coordinates and turns this into features.
    In this case, the standard deviation of movement vertically. Extension to also horizontally can be easily made in case this helps for discovering speed.

    :param coord_df: A dataframe containing all relevant coördiantes observed in the video.

    :return features_df: returns a dataframe containing standard deviations of all observed coordinates
    """
    coord_df['video'] = video_number  # needs to be used as itterator in later version for multiple video's

    y_df = coord_df.pivot_table(index=['video', 'Fragment'], columns='Point', values='y', aggfunc=np.std)
    # y_df.columns = [str(col) + '_y' for col in y_df.columns]
    y_df['video'] = y_df.index
    feature_df = y_df

    return feature_df, coord_df
    # return y_df


def forward_leaning(feature_df, coord_df):
    """
    Create forward leaning feature to be used in classification. The forward leaning feature discribes to what extent a person
    leans forward. which could be an indicator of a good runner

    :param feature_df: feature_df
    :param coord_df:  A dataframe containing all relevant coördiantes observed in the video.
    :return feature_df: returns a dataframe containing standard deviations and other features of all observed coordinates
    """
    feature_df['Forward_leaning'] = 0
    fragments = set(coord_df.Fragment)

    for i in range(len(fragments)):
        fragment_df = coord_df[coord_df['Fragment'] == i + 1]
        shoulder_df = fragment_df[fragment_df['Point'] == 'Right Shoulder']
        hip_df = fragment_df[fragment_df['Point'] == 'Right Hip']
        frames = set(fragment_df.Frame)
        temp_sum = 0
        frame_count = 0
        for j in range(len(frames)):
            difference = shoulder_df.iloc[j, 3] - hip_df.iloc[j, 3]
            # couldn't think of a smarter way to not take the nan values into account for the average
            if difference > 1:
                frame_count += 1
                temp_sum += difference
            if difference < -1:
                frame_count += 1
                temp_sum += difference
            # print(difference)
        # print(temp_sum)
        feature_df.iloc[i, 19] = abs(temp_sum / frame_count)
    return feature_df


def angle_between(p1, p2):
    ang1 = np.arctan2(*p1[::-1])
    ang2 = np.arctan2(*p2[::-1])
    return np.rad2deg((ang1 - ang2) % (2 * np.pi))


def forward_leaning_angle(coord_df):
    """
    Create forward leaning feature to be used in classification. The forward leaning feature describes to what extent a person
    leans forward. which could be an indicator of a good runner

    :param coord_df: A dataframe containing all relevant coördiantes observed in the video.
    :param running_fragments: Running intervals for a given video
    :return forward_leaning_per_fragment: Return a list with a forward leaning angle for each fragment
    """

    forward_leaning = []

    fragments = coord_df['Fragment'].unique()  # get all running fragments

    for fragment in fragments:
        fragment_df = coord_df[coord_df['Fragment'] == fragment]

        start = fragment_df[fragment_df['Frame'] == fragment_df['Frame'].min()]['x'].mean()  # start x
        end = fragment_df[fragment_df['Frame'] == fragment_df['Frame'].max()]['x'].mean()  # end x

        forward_leaning.append([])

        frames = fragment_df['Frame'].unique()  # unique frames for this fragment

        for frame in frames:
            df_sel = fragment_df[fragment_df['Frame'] == frame]
            forward_leaning_angles = []
            for points in [('Right Shoulder', 'Right Hip'), ('Left Shoulder', 'Left Hip')]:
                coords = df_sel[df_sel['Point'].isin(points)][['x', 'y']].as_matrix()
                if len(coords) == 2:
                    forward_leaning_point = coords[0] - coords[1]

                    # Determine direction
                    if end > start:  # direction is right
                        forward_leaning_angles.append(
                            angle_between((forward_leaning_point[0], forward_leaning_point[1]),
                                          (abs(forward_leaning_point[0]), 0)))
                    else:  # direction is left
                        forward_leaning_angles.append(angle_between((-abs(forward_leaning_point[0]), 0),
                                                                    (forward_leaning_point[0],
                                                                     forward_leaning_point[1])))
            if forward_leaning_angles != []:  # If points were found in this frame
                forward_leaning[fragment].append(np.mean(forward_leaning_angles))

    forward_leaning_per_fragment = [np.mean(forward_leaning_list) for forward_leaning_list in forward_leaning]

    return forward_leaning_per_fragment


# Plotly functions to make coördinates more insightfull
# todo: @collin kan jij nog naar deze functie kijken
def plotly_scatterplot(pointlist, coord_df):
    points = []
    for i in range(len(pointlist)):
        pointdf = coord_df[(coord_df['Point'] == pointlist[i])]
        trace = go.Scatter(
            # df = worldbank[(worldbank['Country'] == 'Belgium')],
            x=pointdf['x'],
            y=pointdf['y'],
            mode='markers',
            name=pointlist[i],
            # text=pointdf['Frame'],
            opacity=0.7,
            marker=dict(
                size='5',  # makes the dots invisible, can't get rid of them somehow
                color=i
            )
        )
        points.append(trace)

    layout = dict(
        title='Open Pose coordinate tracker',
        hovermode='closest',
        yaxis=dict(
            #         rangeslider=dict(),
            #         type='date'
            # range=[-600, -300]
        )
        #     ylim = (-600, 300)
    )

    fig = dict(data=points, layout=layout)

    return fig


def plotly_boxplot(pointlist, coord_df):
    # TODO: @collin Why do we do almost the same thing twice?
    points = []
    for i in range(len(pointlist)):
        pointdf = coord_df[(coord_df['Point'] == pointlist[i])]
        trace = go.Box(
            # df = worldbank[(worldbank['Country'] == 'Belgium')],
            y=pointdf['y'],
            # boxpoints = 'all',
            name=pointlist[i],
            #         text = pointdf['Frame'],
            opacity=0.7,
            #         marker=dict(
            #             size='5', #makes the dots invisible, can't get rid of them somehow
            #             color = i
            #         )
        )
        points.append(trace)

    layout = dict(
        title='Open Pose',
        hovermode='closest',
        yaxis=dict(
            #         rangeslider=dict(),
            #         type='date'
            # range=[-600, -300]
        )
        #     ylim = (-600, 300)
    )

    fig = dict(data=points, layout=layout)

    return fig


# Obsoleet functions

def join_lists_on_mutual_elements(plottable_subsets):
    """


    :param plottable_subsets:
    :return:
    """
    all_moving_people = set(chain.from_iterable(plottable_subsets))
    for each in all_moving_people:
        components = [x for x in plottable_subsets if each in x]
        for i in components:
            plottable_subsets.remove(i)
        plottable_subsets += [list(set(chain.from_iterable(components)))]
    return plottable_subsets


def get_links(moving_people, mean_x_per_moving_person):
    links = []
    for n, person in enumerate(moving_people):
        x = mean_x_per_moving_person[person][:, 0]
        y = mean_x_per_moving_person[person][:, 1]

        # calculate polynomial
        z = np.polyfit(x, y, 1)
        f = np.poly1d(z)

        i = 2  # how many periods back and forth to compare

        if n < i:
            i = n

        for j in range(1, i + 1):
            if n > 0:
                previous_person = moving_people[n - j]

                previous_x = mean_x_per_moving_person[previous_person][:, 0]
                previous_y = mean_x_per_moving_person[previous_person][:, 1]

                previous_rmse = rmse(f(previous_x), previous_y)

                links.append((previous_person, person, previous_rmse))

            if n < len(moving_people) - j:
                next_person = moving_people[n + j]

                next_x = mean_x_per_moving_person[next_person][:, 0]
                next_y = mean_x_per_moving_person[next_person][:, 1]

                next_rmse = rmse(f(next_x), next_y)

                links.append((person, next_person, next_rmse))
    return links


# Averaging RMSE between links
def get_linked_people(maximum_normalized_distance, links):
    """

    """
    link_rmse = np.array(
        [(key, np.mean(np.array(list(group))[:, 2])) for key, group in groupby(links, lambda x: (x[0], x[1]))])
    # Use threshold on RMSE to get linked people
    linked_people = link_rmse[link_rmse[:, 1] < maximum_normalized_distance * 2][:, 0]

    # Setting in right format
    return [list(i) for i in linked_people]


# Getting plottable information per file
# Getting plottable information per file
def get_plottables_per_file_and_period_person_division(people_per_file, fps, connections):
    """"
    Troep code die we nog ff niet begrijpen maar het werkt haleluja

    """
    plottables_per_file = []  # used for plotting all the coordinates and connected body part lines

    # for each period/frame all 'people' are stored.
    # For a certain period this will allow us to look back in time at the previous x frames
    # in order to be able to group people in disjoint frames together

    period_person_division = {}  # Dict of dicts

    next_person = 0
    for frame, file in enumerate(people_per_file):
        period_person_division[
            frame] = {}  # for a frame (period) make a new dictionary in which to store the identified people

        plot_lines = []  # for plotting the entire video
        plot_coords = []  # for plotting the entire video
        plottables = {}  # new dictionary for a period, used for plotting entire video

        # coordinates of all people in this frame will be added to this list, to be iterated over later on
        # for plotting entire video
        coords = []
        # Identifying people over disjoint frames
        period_person_division, next_person = identify_people_over_multiple_frames(fps, frame, period_person_division,
                                                                                   next_person)

        # For plotting the entire video ###
        for person_coords in coords:  # for all people in this frame
            plot_coords = plot_coords + list(
                person_coords[~(person_coords == 0).any(axis=1)])  # append present plottable coords

            # enumerate all x,y coordinate sets to be able to draw up lines
            # remove the ones that contain the value 0 --> joint not present
            coord_dict = {key: value for key, value in dict(enumerate(person_coords[:, :2])).items() if 0 not in value}

            present_keypoints = set(coord_dict.keys())  # only use joints that are present

            # get present connections: a connection contains 2 unique points, if a connection contains one of the keypoints that
            # is not present, the intersection of the connection with the present keypoints will be lower than 2
            # hence we end up with only the present connections
            present_connections = [connection for connection in connections if
                                   len(present_keypoints & set(connection)) == 2]

            # gather the connections, change the layout to fit matplotlib and extend the plot_lines list
            plot_lines = plot_lines + [np.transpose([coord_dict[a], coord_dict[b]]) for a, b in present_connections]

        if len(plot_coords) == 0:
            continue

        plot_coords = np.array(plot_coords)  # for easy indexing

        plottables['plot_coords'] = plot_coords
        plottables['plot_lines'] = plot_lines

        plottables_per_file.append(
            plottables)  # append plottables_per_file with the plottables dictionary for this frame
    return plottables_per_file, period_person_division


def identify_people_over_multiple_frames(fps, file, frame, period_person_division, next_person):
    """
    This

    :param next_person: arbitrary identifier for a person
    :param empty_joints: List of all joints (identified by their index in the coordinate
    :param fps: number of frames per second in current video
    :param frame: current frame in loop

    :param person_coords: Coordinates identified by openpose for this person in current frame

    :return
    """
    # next_person = 0  # used to create a new person when the algorithm can't find a good person fit based on previous x frames
    for person in file:
        # information for identifying people over disjoint frames
        person_coords = np.array([[x, -y, z] for x, y, z in np.reshape(person['pose_keypoints'], (18, 3))])

        best_person_fit = None  # Initially no best fit person in previous x frames is found
        if frame == 0:  # period == 0 means no identified people exist, so we need to create them ourselves
            period_person_division[frame][
                next_person] = person_coords  # create new next people since it is the first period
            next_person += 1
        else:
            # set sufficiently high rmse so it will be overwritten easily
            min_rmse = 1000

            # we don't want to base any computation on joints that are not present (==0), so we safe those indices that don't
            # contain any information
            empty_joints = set(np.where((person_coords == 0).all(axis=1))[0])

            # only select used joints
            used_joints = list(set(range(18)) - empty_joints)
            # set rmse_threshold equal to the mean distance of each used joint to the center
            rmse_threshold = determine_rmse_threshold(person_coords, used_joints)

            # for all possible previous periods within max_frame_diff
            for i in range(1, amount_of_frames_to_look_back(fps, frame) + 1):
                for earlier_person in period_person_division[frame - i].keys():  # compare with all people
                    if earlier_person not in period_person_division[
                        frame].keys():  # if not already contained in current period
                        earlier_person_coords = period_person_division[frame - i][earlier_person]
                        empty_joints_copy = empty_joints.copy()
                        empty_joints_copy = empty_joints_copy | set(
                            np.where((earlier_person_coords == 0).all(axis=1))[0])
                        used_joints = list(set(range(18)) - empty_joints_copy)
                        if len(used_joints) == 0:
                            continue
                        # compute root mean squared error based only on mutual used joints
                        person_distance = rmse(earlier_person_coords[used_joints, :], person_coords[used_joints, :])
                        if person_distance < rmse_threshold:  # account for rmse threshold (only coordinates very close)
                            if person_distance < min_rmse:  # if best fit, when compared to previous instances
                                min_rmse = person_distance  # overwrite
                                best_person_fit = earlier_person  # overwrite
            if best_person_fit is not None:  # if a best person fit is found
                period_person_division[frame][best_person_fit] = person_coords
            else:  # else create new next person
                period_person_division[frame][next_person] = person_coords
                next_person += 1
    return period_person_division, next_person
