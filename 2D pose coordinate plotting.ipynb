{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First run:\n",
    "# bin\\OpenPoseDemo.exe --video=\"C:\\Users\\jaspe\\tf-openpose\\clips\\20180205_185116.mp4\" --write_json=coordinates --keypoint_scale=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "#import cv2 \n",
    "# pip install .whl file from https://www.lfd.uci.edu/~gohlke/pythonlibs/#opencv\n",
    "# pip install numpy --upgrade if numpy.multiarray error\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "from itertools import chain\n",
    "from itertools import groupby\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(a,b):\n",
    "    return sqrt(mean_squared_error(a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = cv2.VideoCapture(r'C:\\Users\\jaspe\\tf-openpose\\clips\\20180205_185116.mp4')\n",
    "ret_val, image = cam.read()\n",
    "image_h, image_w = image.shape[:2] # getting clip resolution using opencv\n",
    "fps = cam.get(cv2.CAP_PROP_FPS) # getting clip frames per second using opencv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manually set resolution\n",
    "image_h = 1080\n",
    "image_w = 1920\n",
    "fps = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connections = [\n",
    "    (1,2), (1,5), (2,3), (3,4), (5,6), (6,7), (1,8), (8,9), (9,10), (1,11), (11,12), (12,13), (1,0), (0,14), (14,16),\n",
    "    (0,15), (15,17), (2,16), (5,17)\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_per_file = []\n",
    "\n",
    "# coordinate files are ordered, so we can iterate through the folder in which the coordinates are stores for a clip\n",
    "# each file corresponds to a frame\n",
    "\n",
    "for path, subdirs, files in os.walk(r'C:\\Users\\jaspe\\tf-openpose\\demo\\openpose-1.2.1-win64-binaries\\coordinates'):\n",
    "    for filename in files:\n",
    "        coord_path = os.path.join(path, filename)\n",
    "        with open(coord_path) as f:\n",
    "            people_per_file.append(json.load(f)['people'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manually load people per file \n",
    "with open('people_per_file_clip_20180205_185116.pickle', 'rb') as file:\n",
    "    people_per_file = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting plottable information per file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plottables_per_file = [] # used for plotting all the coordinates and connected body part lines\n",
    "\n",
    "# for each period all 'people' are stored. For a certain period this will allow us to look back in time at the previous x frames\n",
    "# in order to be able to group people in disjoint frames together\n",
    "\n",
    "period_person_division = {}  \n",
    "next_person = 0 # used to create a new person when the algorithm can't find a good person fit based on previous x frames\n",
    "\n",
    "for period, file in enumerate(people_per_file):\n",
    "    period_person_division[period] = {} # for a frame (period) make a new dictionary in which to store the identified people\n",
    "    \n",
    "    plot_lines = [] # for plotting the entire video\n",
    "    plot_coords = [] # for plotting the entire video\n",
    "    plottables = {} # new dictionary for a period, used for plotting entire video\n",
    "    \n",
    "    # coordinates of all people in this frame will be added to this list, to be iterated over later on\n",
    "    # for plotting entire video\n",
    "    coords = [] \n",
    "    \n",
    "    for person in file:\n",
    "        # append coords for this frame/file for each person in the right format\n",
    "        coords.append(np.array([[x,-y,z] for x,y,z in np.reshape(person['pose_keypoints'], (18,3))]))\n",
    "        \n",
    "        # information for identyfing people over disjoint frames\n",
    "        person_coords = np.array([[x,-y,z] for x,y,z in np.reshape(person['pose_keypoints'], (18,3))])\n",
    "        # we don't want to base any computation on joints that are not present (==0), so we safe those indices that don't \n",
    "        # contain any information\n",
    "        empty_joints = set(np.where((person_coords==0).all(axis=1))[0])\n",
    "        \n",
    "        ### Identifying people over disjoint frames ###\n",
    "        \n",
    "        best_person_fit = None # Initially no best fit person in previous x frames is found\n",
    "        if period != 0: # period == 0 means no identified people exist, so we need to create them ourselves\n",
    "            min_rmse = 1000 # set sufficiently high rmse so it will be overwritten easily\n",
    "            used_joints = list(set(range(18))-empty_joints) # only select used joints\n",
    "            cx = np.mean(person_coords[used_joints,0]) # center x-coordinate\n",
    "            cy = np.mean(person_coords[used_joints,1]) # center y-coordinate\n",
    "            # set rmse_threshold equal to the mean distance of each used joint to the center\n",
    "            rmse_threshold = np.mean(pairwise_distances(np.array((cx, cy)).reshape(1,2), \n",
    "                                                        person_coords[used_joints, :2]))\n",
    "            \n",
    "            max_frame_diff = int(fps//4) # number of frames to look back, set to 0.25 sec rather than number of frames\n",
    "            if period < max_frame_diff:\n",
    "                j = period\n",
    "            else:\n",
    "                j = max_frame_diff\n",
    "                \n",
    "            for i in range(1,j+1): # for all possible previous periods within max_frame_diff\n",
    "                for earlier_person in period_person_division[period-i].keys(): # compare with all people\n",
    "                    if earlier_person not in period_person_division[period].keys(): # if not already contained in current period\n",
    "                        earlier_person_coords = period_person_division[period-i][earlier_person]\n",
    "                        empty_joints_copy = empty_joints.copy()\n",
    "                        empty_joints_copy = empty_joints_copy | set(np.where((earlier_person_coords==0).all(axis=1))[0])\n",
    "                        used_joints = list(set(range(18)) - empty_joints_copy)\n",
    "                        if len(used_joints) == 0:\n",
    "                            continue\n",
    "                        # compute root mean squared error based only on mutual used joints\n",
    "                        person_distance = rmse(earlier_person_coords[used_joints,:], person_coords[used_joints,:])\n",
    "                        if person_distance < rmse_threshold: # account for rmse threshold (only coordinates very close)\n",
    "                            if person_distance < min_rmse: # if best fit, when compared to previous instances\n",
    "                                min_rmse = person_distance # overwrite\n",
    "                                best_person_fit = earlier_person # overwrite\n",
    "            if best_person_fit != None: # if a best person fit is found\n",
    "                period_person_division[period][best_person_fit] = person_coords\n",
    "            else: # else create new next person\n",
    "                period_person_division[period][next_person] = person_coords\n",
    "                next_person += 1\n",
    "        else: # create new next people since it is the first period\n",
    "            period_person_division[period][next_person] = person_coords\n",
    "            next_person += 1\n",
    "    \n",
    "    ### For plotting the entire video ###\n",
    "    \n",
    "    for person_coords in coords: # for all people in this frame\n",
    "        \n",
    "        plot_coords = plot_coords + list(person_coords[~(person_coords==0).any(axis=1)]) # append present plottable coords\n",
    "        \n",
    "        # enumerate all x,y coordinate sets to be able to draw up lines\n",
    "        # remove the ones that contain the value 0 --> joint not present\n",
    "        coord_dict = {key:value for key, value in  dict(enumerate(person_coords[:, :2])).items() if 0 not in value}\n",
    "\n",
    "        present_keypoints = set(coord_dict.keys()) # only use joints that are present\n",
    "\n",
    "        # get present connections: a connection contains 2 unique points, if a connection contains one of the keypoints that\n",
    "        # is not present, the intersection of the connection with the present keypoints will be lower than 2\n",
    "        # hence we end up with only the present connections\n",
    "        present_connections = [connection for connection in connections if len(present_keypoints&set(connection)) == 2]\n",
    "        \n",
    "        # gather the connections, change the layout to fit matplotlib and extend the plot_lines list\n",
    "        plot_lines = plot_lines + [np.transpose([coord_dict[a], coord_dict[b]]) for a,b in present_connections]\n",
    "        \n",
    "    if len(plot_coords) == 0:\n",
    "        continue\n",
    "    \n",
    "    plot_coords = np.array(plot_coords) # for easy indexing\n",
    "    \n",
    "    plottables['plot_coords'] = plot_coords\n",
    "    plottables['plot_lines'] = plot_lines\n",
    "    \n",
    "    plottables_per_file.append(plottables) # append plottables_per_file with the plottables dictionary for this frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fit(plottables_per_file, period, f, ax):\n",
    "    \n",
    "    plot_coords = plottables_per_file[period]['plot_coords']\n",
    "    plot_lines = plottables_per_file[period]['plot_lines']\n",
    "    \n",
    "    plt.scatter(x=plot_coords[:, 0], y=plot_coords[:, 1])\n",
    "\n",
    "    for x, y in plot_lines:\n",
    "        plt.plot(x, y)\n",
    "\n",
    "    ax.set_xlim([0, image_w])\n",
    "    ax.set_ylim([-image_h, 0])\n",
    "    \n",
    "    f.canvas.draw()\n",
    "    ax.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# f, ax = plt.subplots(figsize=(14,10))\n",
    "# xspeed = 4\n",
    "\n",
    "# for t in range(0, len(plottables_per_file)):\n",
    "#     plot_fit(plottables_per_file, period=t, f=f, ax=ax)\n",
    "# #     time.sleep(1/fps/xspeed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting person under observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Identifying moving people*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basically change the layout of the dictionary\n",
    "# Now you first index based on the person and then you index based on the period\n",
    "\n",
    "person_period_division = {}\n",
    "for person in set(chain.from_iterable(period_person_division.values())):\n",
    "    person_period_division[person] = {}\n",
    "    for period in period_person_division.keys():\n",
    "        period_dictionary = period_person_division[period]\n",
    "        if person in period_dictionary:\n",
    "            person_period_division[person][period] = period_dictionary[person]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean x-position of a person in a certain period\n",
    "\n",
    "mean_x_per_person = {person:{period:np.mean(coords[~(coords==0).any(axis=1),0]) \n",
    "                             for period,coords in time_coord_dict.items()} \n",
    "                             for person,time_coord_dict in person_period_division.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate moved distance by summing the absolute difference over periods\n",
    "# Normalize moved distance per identified person over frames by including the average frame difference and the length\n",
    "# of the number of frames included\n",
    "\n",
    "normalized_moved_distance_per_person = \\\n",
    "{person:pd.Series(mean_x_dict).diff().abs().sum()/(np.diff(pd.Series(mean_x_dict).index).mean()*len(mean_x_dict)) \n",
    "     for person, mean_x_dict in mean_x_per_person.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only include identified people that move more than a set movement threshold\n",
    "\n",
    "maximum_normalized_distance = max(normalized_moved_distance_per_person.values())\n",
    "movement_threshold = maximum_normalized_distance/4\n",
    "moving_people = [key for key,value in normalized_moved_distance_per_person.items() if value > movement_threshold]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Finding person under observation based on clustering with DBSCAN*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_plottables_df = \\\n",
    "pd.DataFrame([(period, person, x) for person,period_dict in mean_x_per_person.items() if person in moving_people \n",
    "              for period,x in period_dict.items()], columns=['Period', 'Person', 'X mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({key:value for key,value in mean_x_per_person.items() if key in moving_people}).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "db = DBSCAN(eps=maximum_normalized_distance*2, min_samples=1)\n",
    "\n",
    "db.fit(person_plottables_df[['Period', 'X mean']])\n",
    "\n",
    "person_plottables_df['labels'] = db.labels_\n",
    "\n",
    "maximum_label = person_plottables_df.groupby('labels').apply(len).sort_values(ascending=False).index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DBSCAN_subsets = person_plottables_df.groupby('labels')['Person'].unique().tolist()\n",
    "\n",
    "DBSCAN_subsets = [list(i) for i in DBSCAN_subsets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Supplementing DBSCAN result with person-specific extrapolation and matching based on RMSE*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_x_per_moving_person = {key:np.array([[period,x] for period,x in value.items()]) \n",
    "                            for key,value in mean_x_per_person.items() if key in moving_people}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = []\n",
    "for n,person in enumerate(moving_people):\n",
    "    x = mean_x_per_moving_person[person][:,0]\n",
    "    y = mean_x_per_moving_person[person][:,1]\n",
    "\n",
    "    # calculate polynomial\n",
    "    z = np.polyfit(x, y, 1)\n",
    "    f = np.poly1d(z)\n",
    "    \n",
    "    if n > 0:\n",
    "    \n",
    "        previous_person = moving_people[n-1]\n",
    "\n",
    "        previous_x = mean_x_per_moving_person[previous_person][:,0]\n",
    "        previous_y = mean_x_per_moving_person[previous_person][:,1]\n",
    "\n",
    "        previous_rmse = rmse(f(previous_x), previous_y)\n",
    "\n",
    "        links.append((previous_person, person, previous_rmse))\n",
    "    \n",
    "    if n < len(moving_people)-1:\n",
    "    \n",
    "        next_person = moving_people[n+1]\n",
    "\n",
    "        next_x = mean_x_per_moving_person[next_person][:,0]\n",
    "        next_y = mean_x_per_moving_person[next_person][:,1]\n",
    "\n",
    "        next_rmse = rmse(f(next_x), next_y)\n",
    "\n",
    "        links.append((person, next_person, next_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Averaging RMSE between links\n",
    "link_rmse = np.array([(key, np.mean(np.array(list(group))[:,2])) for key,group in groupby(links, lambda x: (x[0], x[1]))])\n",
    "\n",
    "# Use threshold on RMSE to get linked people\n",
    "linked_people = link_rmse[link_rmse[:,1]<maximum_normalized_distance*2][:,0]\n",
    "\n",
    "# Setting in right format\n",
    "linked_people = [list(i) for i in linked_people]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge lists that share common elements\n",
    "\n",
    "plottable_subsets = DBSCAN_subsets + linked_people\n",
    "\n",
    "all_moving_people = set(chain.from_iterable(plottable_subsets)) \n",
    "\n",
    "for each in all_moving_people:\n",
    "    components = [x for x in plottable_subsets if each in x]\n",
    "    for i in components:\n",
    "        plottable_subsets.remove(i)\n",
    "    plottable_subsets += [list(set(chain.from_iterable(components)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plottable_people = \\\n",
    "plottable_subsets[np.argmax([sum([len(person_period_division[person]) for person in subset]) for subset in plottable_subsets])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Plot person under observation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_plottables = [{key:value for key,value in period_dictionary.items() if key in plottable_people}\n",
    "                    for period,period_dictionary in period_person_division.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_plottables = list(filter(lambda x: x != {}, person_plottables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_person(plottables, f, ax):\n",
    "    \n",
    "    for person in plottables.keys():\n",
    "        plot_coords = plottables[person]\n",
    "\n",
    "        coord_dict = {key:value for key, value in dict(enumerate(plot_coords[:,:2])).items() if 0 not in value}\n",
    "\n",
    "        present_keypoints = set(coord_dict.keys())\n",
    "\n",
    "        present_connections = [connection for connection in connections if len(present_keypoints&set(connection)) == 2]\n",
    "\n",
    "        plot_lines = [np.transpose([coord_dict[a], coord_dict[b]]) for a,b in present_connections]\n",
    "\n",
    "        plot_coords = plot_coords[~(plot_coords==0).any(axis=1)]\n",
    "\n",
    "        plt.scatter(x=plot_coords[:, 0], y=plot_coords[:, 1])\n",
    "\n",
    "        for x, y in plot_lines:\n",
    "            plt.plot(x, y)\n",
    "\n",
    "#     ax.set_xlim([plot_coords[:,0].min()-100, plot_coords[:,0].max()+100])\n",
    "#     ax.set_ylim([plot_coords[:,1].min()-100, plot_coords[:,1].max()+100])\n",
    "    \n",
    "    ax.set_xlim([0, image_w])\n",
    "    ax.set_ylim([-image_h, 0])\n",
    "    \n",
    "    f.canvas.draw()\n",
    "    ax.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(14,10))\n",
    "xspeed = 4\n",
    "\n",
    "for t in range(len(person_plottables)):\n",
    "    plot_person(person_plottables[t], f=f, ax=ax)\n",
    "#     time.sleep(1/fps/xspeed)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
